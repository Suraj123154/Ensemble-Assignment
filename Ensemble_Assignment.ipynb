{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b86bdf1"
      },
      "source": [
        "### Theoretical Questions\n",
        "\n",
        "**Question 1: What is Ensemble Learning in machine learning? Explain the key idea behind it.**\n",
        "\n",
        "**Answer:** Ensemble learning is a machine learning technique that combines the predictions from multiple individual models to improve the overall performance. The key idea is that by combining diverse models, the ensemble can overcome the weaknesses of individual models and produce a more robust and accurate prediction.\n",
        "\n",
        "**Question 2: What is the difference between Bagging and Boosting?**\n",
        "\n",
        "**Answer:** Bagging (Bootstrap Aggregating) and Boosting are two popular ensemble techniques. The main difference lies in how they build and combine the models:\n",
        "\n",
        "- **Bagging:** In Bagging, multiple models are trained independently on different bootstrap samples of the training data. The final prediction is typically the average (for regression) or majority vote (for classification) of the individual model predictions. Bagging reduces variance and helps to prevent overfitting.\n",
        "- **Boosting:** In Boosting, models are trained sequentially. Each new model is trained to correct the errors of the previous models. The models are weighted based on their performance, and the final prediction is a weighted combination of the individual model predictions. Boosting reduces bias and can improve accuracy, but it can be more sensitive to noisy data and outliers.\n",
        "\n",
        "**Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?**\n",
        "\n",
        "**Answer:** Bootstrap sampling is a resampling technique where multiple samples are drawn with replacement from the original dataset. Each bootstrap sample has the same size as the original dataset. In Bagging methods like Random Forest, bootstrap sampling is used to create multiple diverse training sets for the individual models (decision trees in the case of Random Forest). This diversity in the training data leads to diverse trees, which when combined, reduce variance and improve the overall robustness of the model.\n",
        "\n",
        "**Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?**\n",
        "\n",
        "**Answer:** In Bagging methods, since bootstrap samples are drawn with replacement, some data points from the original dataset may not be included in a particular bootstrap sample. These data points are called Out-of-Bag (OOB) samples for that specific model. OOB samples can be used to evaluate the performance of the ensemble model without the need for a separate validation set. The OOB score is calculated by making predictions on the OOB samples for each model and then aggregating these predictions. It provides an estimate of the model's generalization error.\n",
        "\n",
        "**Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.**\n",
        "\n",
        "**Answer:** Feature importance analysis helps to understand which features are most influential in making predictions.\n",
        "\n",
        "- **Single Decision Tree:** Feature importance in a single decision tree is typically calculated based on how much each feature reduces impurity (e.g., Gini impurity or entropy) across the tree. Features that lead to larger impurity reductions are considered more important. However, the importance scores can be unstable and sensitive to small changes in the data or tree structure.\n",
        "- **Random Forest:** In a Random Forest, feature importance is calculated by averaging the feature importance scores across all the individual decision trees in the forest. This averaging process makes the feature importance scores more stable and robust compared to a single decision tree. Features that are consistently important across multiple trees in the forest will have higher overall importance scores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db766d73"
      },
      "source": [
        "### Practical Questions\n",
        "\n",
        "**Question 6: Write a Python program to:**\n",
        "*   Load the Breast Cancer dataset using `sklearn.datasets.load_breast_cancer()`\n",
        "*   Train a Random Forest Classifier\n",
        "*   Print the top 5 most important features based on feature importance scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "009c93bc",
        "outputId": "74b7dde9-0ced-4648-ace4-d745579b7d77"
      },
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X, y)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = pd.Series(rf_classifier.feature_importances_, index=X.columns)\n",
        "\n",
        "# Print the top 5 most important features\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(feature_importances.nlargest(5))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "worst area              0.139357\n",
            "worst concave points    0.132225\n",
            "mean concave points     0.107046\n",
            "worst radius            0.082848\n",
            "worst perimeter         0.080850\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41a4e5b5"
      },
      "source": [
        "**Question 7: Write a Python program to:**\n",
        "*   Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "*   Evaluate its accuracy and compare with a single Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb2a8976",
        "outputId": "59ddf71a-cb70-4726-e32d-2c338ec32db2"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a single Decision Tree\n",
        "single_tree = DecisionTreeClassifier(random_state=42)\n",
        "single_tree.fit(X_train, y_train)\n",
        "single_tree_pred = single_tree.predict(X_test)\n",
        "single_tree_accuracy = accuracy_score(y_test, single_tree_pred)\n",
        "print(f\"Accuracy of a single Decision Tree: {single_tree_accuracy:.4f}\")\n",
        "\n",
        "# Train a Bagging Classifier\n",
        "bagging_classifier = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "bagging_pred = bagging_classifier.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_pred)\n",
        "print(f\"Accuracy of Bagging Classifier: {bagging_accuracy:.4f}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of a single Decision Tree: 1.0000\n",
            "Accuracy of Bagging Classifier: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e51123b9"
      },
      "source": [
        "**Question 8: Write a Python program to:**\n",
        "*   Train a Random Forest Classifier\n",
        "*   Tune hyperparameters `max_depth` and `n_estimators` using `GridSearchCV`\n",
        "*   Print the best parameters and final accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a12e6cf0",
        "outputId": "4b7eb8f2-077b-47aa-c8f0-e2832b5c3b30"
      },
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'n_estimators': [50, 100, 200]\n",
        "}\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Tune hyperparameters using GridSearchCV\n",
        "grid_search = GridSearchCV(rf_classifier, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and final accuracy\n",
        "print(\"Best parameters found:\", grid_search.best_params_)\n",
        "best_rf_model = grid_search.best_estimator_\n",
        "best_rf_pred = best_rf_model.predict(X_test)\n",
        "best_rf_accuracy = accuracy_score(y_test, best_rf_pred)\n",
        "print(f\"Final accuracy with best parameters: {best_rf_accuracy:.4f}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found: {'max_depth': None, 'n_estimators': 200}\n",
            "Final accuracy with best parameters: 0.9649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58b91e20"
      },
      "source": [
        "**Question 9: Write a Python program to:**\n",
        "*   Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset\n",
        "*   Compare their Mean Squared Errors (MSE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d4fa068",
        "outputId": "0b1a2715-9a11-461c-b322-7acfdc60bb58"
      },
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the dataset\n",
        "california_housing = fetch_california_housing()\n",
        "X, y = california_housing.data, california_housing.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Bagging Regressor\n",
        "bagging_regressor = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=10, random_state=42)\n",
        "bagging_regressor.fit(X_train, y_train)\n",
        "bagging_pred = bagging_regressor.predict(X_test)\n",
        "bagging_mse = mean_squared_error(y_test, bagging_pred)\n",
        "print(f\"Mean Squared Error of Bagging Regressor: {bagging_mse:.4f}\")\n",
        "\n",
        "# Train a Random Forest Regressor\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_regressor.fit(X_train, y_train)\n",
        "rf_pred = rf_regressor.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "print(f\"Mean Squared Error of Random Forest Regressor: {rf_mse:.4f}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error of Bagging Regressor: 0.2824\n",
            "Mean Squared Error of Random Forest Regressor: 0.2554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e037621b"
      },
      "source": [
        "**Question 10: You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "● Choose between Bagging or Boosting\n",
        "● Handle overfitting\n",
        "● Select base models\n",
        "● Evaluate performance using cross-validation\n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context.**\n",
        "\n",
        "**Answer:** Here is a step-by-step approach to applying ensemble learning for loan default prediction in a financial institution:\n",
        "\n",
        "**1. Choose between Bagging and Boosting:**\n",
        "\n",
        "*   **Consider the data and problem:** Loan default prediction is a complex problem with potentially noisy data and a need for high accuracy.\n",
        "*   **Evaluate strengths:** Boosting models like Gradient Boosting or XGBoost often perform well on complex datasets and can capture intricate relationships. Bagging models like Random Forest are generally more robust to noisy data and outliers.\n",
        "*   **Initial approach:** Given the potential for complex interactions in financial data, start by exploring Boosting techniques. If overfitting becomes a significant issue or the data is particularly noisy, consider Bagging or a combination of both.\n",
        "\n",
        "**2. Handle overfitting:**\n",
        "\n",
        "*   **Regularization:** Use regularization techniques within the chosen ensemble method. For example, in Gradient Boosting, you can use L1 or L2 regularization.\n",
        "*   **Hyperparameter tuning:** Carefully tune hyperparameters using techniques like GridSearchCV or RandomizedSearchCV. Parameters like `max_depth`, `min_samples_leaf`, and `subsample` can help control model complexity and prevent overfitting.\n",
        "*   **Early stopping:** For iterative methods like Boosting, use early stopping based on a validation set to stop training when the model's performance on the validation set starts to degrade.\n",
        "*   **Cross-validation:** Use cross-validation during training and evaluation to get a more reliable estimate of the model's performance on unseen data.\n",
        "\n",
        "**3. Select base models:**\n",
        "\n",
        "*   **Diversity:** Choose base models that are diverse and capture different aspects of the data. For example, you could use decision trees, linear models, or even simple neural networks as base learners.\n",
        "*   **Complexity:** The complexity of the base models depends on the ensemble technique. For Boosting, simple base models (e.g., shallow decision trees) are often preferred. For Bagging, more complex base models (e.g., deep decision trees) can be used.\n",
        "*   **Interpretability:** Consider the need for interpretability. Decision trees are relatively easy to interpret, which can be important in a financial context.\n",
        "\n",
        "**4. Evaluate performance using cross-validation:**\n",
        "\n",
        "*   **Splitting the data:** Split the data into multiple folds (e.g., 5 or 10).\n",
        "*   **Training and evaluation:** For each fold, train the ensemble model on the training data and evaluate its performance on the validation data (the remaining folds).\n",
        "*   **Metrics:** Use appropriate evaluation metrics for loan default prediction, such as accuracy, precision, recall, F1-score, and AUC (Area Under the ROC Curve). Precision and recall are particularly important to balance the cost of false positives and false negatives.\n",
        "*   **Averaging:** Average the performance metrics across all folds to get a robust estimate of the model's performance.\n",
        "\n",
        "**5. Justify how ensemble learning improves decision-making in this real-world context:**\n",
        "\n",
        "*   **Increased accuracy:** Ensemble learning combines the strengths of multiple models, leading to more accurate predictions of loan default. This can reduce financial losses for the institution by minimizing false negatives (approving loans that default) and false positives (denying loans to creditworthy customers).\n",
        "*   **Robustness:** Ensemble models are generally more robust to noise and outliers in the data, which is common in financial datasets.\n",
        "*   **Reduced variance:** Bagging techniques like Random Forest reduce variance and prevent overfitting, leading to better generalization on unseen data.\n",
        "*   **Improved decision-making:** By providing more accurate and reliable predictions, ensemble learning helps the financial institution make better decisions about loan approvals, risk assessment, and resource allocation. This can lead to improved profitability and reduced risk.\n",
        "*   **Identifying important factors:** Feature importance analysis from ensemble models (especially Random Forest) can help identify the most important factors influencing loan default, providing valuable insights for business decisions and risk management strategies."
      ]
    }
  ]
}